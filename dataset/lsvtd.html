<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="index, follow">

    <title>DAVAR LAB</title>
    <meta name="description" content="">
    <link rel="alternate" type="application/rss+xml" href="/feed.xml">

    <!-- Stylesheet -->
    <link rel="stylesheet" href="/css/main.css" rel='stylesheet' type='text/css'>

    <!-- Google Fonts -->
    <link href='https://fonts.googleapis.com/css?family=Nunito:400,700' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic' rel='stylesheet'
        type='text/css'>

    <!-- Favicon -->
    <link rel="shortcut icon" href="/img/small_logo.png">
</head>

<body data-spy="scroll" data-offset="80" data-target=".scrollspy" id="top">
    <div class="navigation"></div>

    <div class="news top-container">
        <div class="container-fluid">
            <div id="primarycontent">
                <center>
                    <h2>A Large-scale Video Text Dataset</h2>
                </center>
                <center>
                    <h3>
                        <a href="https://davar-lab.github.io/" target="_blank">DAVAR LAB</a>
                    </h3>
                </center>
                <center>
                    <h3>
                        <a href="https://arxiv.org/pdf/1903.03299.pdf"
                            target="_blank" style="color: #990000">[Download (Please email us [See Contact])]</a>
                    </h3>
                </center>
                <center>
                    <img src="/img/dataset/lsvtd/dataset.png" width="1000">
                </center>
                <p></p>


                <p>
                    <h2>Introduction</h2>

                    <div style="font-size:14px">
                        <p>Here, we release a large-scale video text dataset named LSVTD. In recent years, research in video scene text still remains unpopular in contrast to its promising application prospect. The existing video scene text datasets are limited on the scale of video items and scenarios, which may restrain research of video scene text spotting. Then we collect and annotate LSVTD, which contains 100 scene videos acquired from 21 typical real-life scenarios.
						
                        </p>

                        <p>
                            Note that, in the origin paper, we have decaleard that LSVTD is collected from 22 scenes. However, due to some privacy policy, we have to remove 1 video text scenario (including 5 videos), leading to 21 scenarios with 95 videos keeped. Even so, we then supplement 2, 1 and 2 videos for city road, street view and bookstore scenarios, respectively. Besides, we also refine the dataset by removing some segments of background frames without text content.
                        </p>

                        <p>
                            LSVTD mainly characterized is described in detail as follows:
                        </p>

                        <p>
                            <li> <strong>Much larger scale.</strong> LSVTD has 100 videos, consists of more than 65k frames and 563k instances.</li>
							<li> <strong>More diversified scenarios.</strong> LSVTD covers a wide range of 13 indoor (eg. bookstore, shopping mall) and 8 outdoor (eg. highway, city road) scenarios. The variety of scenarios challenges text spotting algorithms to achieve robust performance.</li>
							<li> <strong>Multilingual text instances.</strong> LSVTD contains text with more than two kinds of languages (English and Chinese etc.), which are further divided into 2 major categories: Latin and Non-Latin. We additionally label this attribute for the convenience of evaluation for different algorithms. In addition, each text region is labeled with quality score.</li>

                        </p>

                    </div>

                    <h2>Dataset released</h2>
                    <p>
                        <li> <strong>Videos</strong>: 100 videos are provided in total, in which 66 videos for training and 34 videos for testing.</li>
						
						<li> <strong>Annotations</strong>: only the training annotations (.xml) are provided.</li>
						
						<p> If you need evaluate your algorithm on the testing set, please send us the predicted results (same as the annotation format) by email. We will give you the evaluation results as soon as possible. In future, we will consider to add the on-line evaluation server or provide a evaluation tool.</p>
						
						<li><strong>Dataset request</strong>: If you need this dataset, please email us [See Contact]. We will share a download link with you. Tips: Please tell us your name and institute in your email.</li>
                    </p>

                    <h2>Contact</h2>
					<p>If you have any questions about the dataset, please contact Jing Lu or Zhanzhan Cheng .(lujing6kh@163.com or 11821104@zju.edu.cn).</p>
					
					<h2>Terms of Use</h2>
					
					<li> The public annotations belong to Zhejiang University and are licensed under the <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>. </li>
					<li> The images belong to Zhejiang University and are licensed under the <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</li>
					
					
					<h2> Change Log</h2>
					<li>2020-03-06 (GMT+8): dataset update by removing some consecutive background frames and adding 5 extra videos</li>
					<li>2019-10-25 (GMT+8): dataset released</li>
					
					
                    <h2>Recommended Citation</h2>
                    If you find this dataset is helpful to your research, please feel free to cite us:

                    <pre>
@inproceedings{cheng2019you, 
			title={You Only Recognize Once: Towards Fast Video Text Spotting}, 
			author={Cheng, Zhanzhan and Lu, Jing and Niu, Yi and Pu, Shiliang and Wu, Fei and Zhou, Shuigeng}, 
			booktitle={Proceedings of the 27th ACM International Conference on Multimedia}, 
			pages={855â€“863}, 
			year={2019}, 
			organization={ACM} 
			}
}</pre>
                </p>
            </div>
        </div>
    </div>

    <div class="footer"></div>
    <script src="/js/jquery.min.js"></script>
    <script src="/js/all.min.js"></script>

    <script>
        $(document).ready(function () {
            $(".footer").load("/common/footer.html");
            $(".navigation").load("/common/navigation.html");
        });
    </script>

</body>

</html>
